\section{Motivations}
The analysis of animal behaviour is essential for wildlife conservation and management \parencite{Greggor2019, singh2020animal}. To conduct long-run and consistent animal behaviour analysis, researchers usually utilise video cameras to do the action recognition. This helps them to gain a better understanding of animal populations and ecosystems, health and needs, movement-related injuries, and changes in their behaviours \parencite{ng2022animal, Giersberg:2022aa, 8259762}.

Due to the the wide range of temporal scale across various animal actions, video footage makes it possible for continuous monitoring of multiple animals at different time scale \parencite{ANDERSON201418}. However, unlike human action recognition, the animal action videos are usually collected in the wild and a wide variety of actions performed by different animal species with various sizes, textures, shapes as well as motion speed make them more difficult to be correctly recognised. Unfortunately, as the majority of researches focused on the human action recognition, animal action recognition has not been well studied \parencite{mondal2023msqnet}.

\section{Challenges}
\subsection{Long-Tail Issue}
The first challenge in animal action recognition is the long-tail issue. Since the collection of animal behaviour footage are generally conducted in the wild, animal action datasets usually contain imbalanced data for different actions, reflecting the natural distribution. While the head classes of actions are most likely seen in nature, the middle and tail classes are less frequent and may be observed with a very tiny probability. As a consequence, collecting action videos in the tail classes is costly, leading to insufficient samples in the tail \parencite{ng2022animal, perrett2023use}. With the natural of deep learning being able to learn only from the data, the long tail issue has been a huge challenge for network training, leading to the poor performance on less frequent classes \parencite{cao2019learning, zhang2021videolt}.

% TODO: adjust the size of the text in the figure
\begin{figure}[h]
    \centering
    % \includegraphics[width=1\textwidth]{assets/imgs/1_1_ClassEmbeddingInternVideo}
    \adjustbox{trim=6.2cm 2cm 5cm 2cm}{%
        \resizebox{1.2\textwidth}{!}{\input{"assets/imgs/1_1_ClassEmbeddingInternVideo.pgf"}}
    }
    \caption[Class Embedding Distribution]{This figure shows the distribution of class embeddings. The 140 action classes are converted into sentence using the prompt template "A video of an animal <action>. This action is a kind of <category of action> behaviour.", which then encoded by the text encoding module of InternVideo \parencite{wang2022internvideo} pre-trained on k400 dataset. The Principle Component Analysis is applied to reduce the dimension to be ploted in the 2D plane. Green labels represent the head classes. Blue labels represent middle classes, while red labels represent the tail classes.}
    \label{fig:1_1_ClassEmbeddingInternVideo}
\end{figure}



In this research, I proved that CLIP model is able to resolve this issue effectively. It has been proved that the distance of text embeddings is strongly correlated with the similarity of its semantic meaning, and even able to do the meaningful plus or minus calculation \parencite{mikolov2013efficient}. As the Figure \ref{fig:1_1_ClassEmbeddingInternVideo} shown, applying the principle component analysis on the text embedding generated by video clip module of InternVideo \parencite{wang2022internvideo} enable us to observe some relationship between classes. For example, the movement actions such as walking, running, flying and landing are clustered in the right-bottom of the figure, while the birth related actions such as giving birth, laying eggs, hatching and undergoing chrysalis, are gathering in the top of the figure. Thanks to the CLIP, it is possible to encode the text and visual data into the same semantic space. For action recognition task, it is reasonable to transfers the learning target of the model from one-hot encoding to a class embedding in the semantic space \parencite{ma2022x} to mitigate the unblance distribution caused by lone tail issue. 

\subsection{Temporal Redundancy}

% TODO: change the figure, add caption
% TODO: change name to Prompt Embedding
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{assets/imgs/1_2_FrameComparison}
    \caption[Temporal Redundancy]{This figure shows the high temporal redundancy between neighboring frames. Each row of the figure is the sampled frames from the same video. This illustrate the high temporal redundancy in the video.}
    \label{fig:1_2_FrameComparison}
\end{figure}

The second challenge is the temporal redundancy. Since there are a large amount of slow or subtle animal action motions, the high temporal redundancy may confuse the model to do the recognition correctly \parencite{YUAN2018221, li2022uniformer}. Figure \ref{fig:1_2_FrameComparison} illustrates the high temporal redundancy in animal videos. 

In order to enhance the ability to capture the fine-grained texture difference across neighbouring frames, I proposed Action Frame Contrastive Learning Network (AFRICAN) to force the model to extract different features from different frames in the same video. This network is used as a counterpart of CLIP which focuses more on the semantic meaning, contributing around 1 mAP to the final result.

% TODO (Backbone): the improved score may be changed 
\section{Contributions}
In this research, I have the following contributions. Fitst, I proved that CLIP model is able to resolve long-tail issue effectively. Secondly, AFRICAN can be a auxiliary enhancer to help the model gain a better understanding on the temporal dimension.

% May not be written here: improving the overall mAP from 40.92\% using one-hot encoding to 54.76\% using prompt embedding. 




% \begin{itemize}
%   \item If you use \begin{verbatim}\parencite{...}\end{verbatim} you will get just a reference, which may
%     or may not contain names: \parencite{qian2021}.
%   \item If you use \begin{verbatim}\textcite{...}\end{verbatim} you will get the author names as well: \textcite{qian2021}.
% \end{itemize}

% \lipsum[1-2]

% \section{Yes?}

% \subsection{Maybe no}

%   \lipsum[3]

% \subsection{Definitely not}
%   \lipsum[4]
  
% \section{What else?}
%   \lipsum[5-13]
