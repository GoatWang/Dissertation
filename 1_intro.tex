\section{Motivations}
The analysis of animal behaviour is essential for wildlife conservation and management \parencite{Greggor2019, singh2020animal}. To conduct long-run and consistent animal behaviour analysis, researchers usually utilise video cameras to do the action recognition. This helps them to gain a better understanding of animal populations and ecosystems, health and needs, movement-related injuries, and changes in their behaviours \parencite{ng2022animal, Giersberg:2022aa, 8259762}.

Due to the wide range of temporal scales across various animal actions, video footage makes it possible for continuous monitoring of multiple animals at different time scales \parencite{ANDERSON201418}. However, unlike human action recognition, animal action videos are usually collected in the wild and a wide variety of actions performed by different animal species with various sizes, textures, shapes as well as motion speeds make them more difficult to recognise correctly. Unfortunately, as the majority of research focused on human action recognition, animal action recognition has not been well studied \parencite{mondal2023msqnet}.

\section{Challenges}
\subsection{Long-Tail Issue}
The first challenge in animal action recognition is the long-tail issue. Since the collection of animal behaviour footage is generally conducted in the wild, animal action datasets usually contain imbalanced data for different actions, reflecting the natural distribution. While the head classes of actions are most likely seen in nature, the middle and tail classes are less frequent and may be observed with a very tiny probability. As a consequence, collecting action videos in the tail classes is costly, leading to insufficient samples in the tail \parencite{ng2022animal, perrett2023use}. With the natural of deep learning being able to learn only from the data, the long tail issue has been a huge challenge for network training, leading to poor performance on less frequent classes \parencite{cao2019learning, zhang2021videolt}.

% TODO: adjust the size of the text in the figure
\begin{figure}[ht]
    \centering
    % \includegraphics[width=1\textwidth]{assets/imgs/1_1_ClassEmbeddingInternVideo}
    \adjustbox{trim=6.2cm 2cm 5cm 2cm}{%
        \resizebox{1.2\textwidth}{!}{\input{"assets/imgs/1_1_ClassEmbeddingInternVideo.pgf"}}
    }
    \caption[Class Embedding Distribution]{This figure shows the distribution of class embeddings. The 140 action classes are converted into sentences using the prompt template "A video of an animal <action>. This action is a kind of <category of action> behaviour.", which then encoded by the text encoding module of InternVideo \parencite{wang2022internvideo} pre-trained on k400 dataset. The Principle Component Analysis is applied to reduce the dimension to be plotted in the 2D plane. Green labels represent the head classes. Blue labels represent the middle classes, while red labels represent the tail classes.}
    \label{fig:1_1_ClassEmbeddingInternVideo}
\end{figure}



In this research, I proved that the CLIP model is able to resolve this issue effectively. It has been proved that the distance of text embeddings is strongly correlated with the similarity of its semantic meaning, and even able to do the meaningful plus or minus calculation \parencite{mikolov2013efficient}. As Figure \ref{fig:1_1_ClassEmbeddingInternVideo} shows, applying the principle component analysis on the text embedding generated by the video clip module of InternVideo \parencite{wang2022internvideo} enables us to observe some relationship between classes. For example, the movement actions such as walking, running, flying and landing are clustered at the right-bottom of the figure, while the birth-related actions such as giving birth, laying eggs, hatching and undergoing chrysalis, are gathered at the top of the figure. Thanks to the CLIP, it is possible to encode the text and visual data into the same semantic space. For the action recognition task, it is reasonable to transfer the learning target of the model from one-hot encoding to a class embedding in the semantic space \parencite{ma2022x} to mitigate the unbalanced distribution caused by the long-tail issue. 

\subsection{Temporal Redundancy}

% TODO: change the figure, add caption
% TODO: change name to Prompt Embedding
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{assets/imgs/1_2_FrameComparison}
    \caption[Temporal Redundancy]{This figure shows the high temporal redundancy between neighbouring frames. Each row of the figure is the sampled frames from the same video. This illustrates the high temporal redundancy in the video.}
    \label{fig:1_2_FrameComparison}
\end{figure}

The second challenge is the temporal redundancy. Since there are a large amount of slow or subtle animal action motions, the high temporal redundancy may confuse the model to do the recognition correctly \parencite{YUAN2018221, li2022uniformer}. Figure \ref{fig:1_2_FrameComparison} illustrates the high temporal redundancy in animal videos. 

In order to enhance the ability to capture the fine-grained texture difference across neighbouring frames, I proposed an Action Frame Contrastive Learning Network (AFRICAN) to force the model to extract different features from different frames in the same video. This network is used as a counterpart of CLIP which focuses more on the semantic meaning, contributing around 1 mAP to the final result.

% TODO (Backbone): the improved score may be changed 
\section{Contributions}
In this research, I have the following contributions. First, I proved that CLIP model is able to resolve the long-tail issue effectively. Secondly, AFRICAN can be an auxiliary enhancer to help the model better understand the temporal dimension.

% May not be written here: improving the overall mAP from 40.92\% using one-hot encoding to 54.76\% using prompt embedding. 




% \begin{itemize}
%   \item If you use \begin{verbatim}\parencite{...}\end{verbatim} you will get just a reference, which may
%     or may not contain names: \parencite{qian2021}.
%   \item If you use \begin{verbatim}\textcite{...}\end{verbatim} you will get the author names as well: \textcite{qian2021}.
% \end{itemize}

% \lipsum[1-2]

% \section{Yes?}

% \subsection{Maybe no}

%   \lipsum[3]

% \subsection{Definitely not}
%   \lipsum[4]
  
% \section{What else?}
%   \lipsum[5-13]
