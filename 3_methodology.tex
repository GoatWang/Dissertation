In this section, the use of CLIP with prompt engineering for animal action recognition is explored to address the long-tail issue, while AFRICAN is introduced as a solution for temporal redundancy. 

\section{CLIP with Prompt Engineering}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{assets/charts/3_1_ModelStructureVC}
    \caption[VideoCLIP model structure]{Illustration of the VideoCLIP model structure.}
    \label{fig:modelstructure_vc}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{assets/charts/3_2_ModelStructureIC}
    \caption[ImageCLIP Model Structure]{Illustration of the ImageCLIP model structure.}
    \label{fig:modelstructure_ic}
\end{figure}

Two model structures are employed to assess action recognition performance: 
\begin{enumerate}
    \item \textbf{VideoCLIP}: CLIP pretrained on videos. The video CLIP module of InternVideo \parencite{wang2022internvideo} is used here. The entire structure is illustrated in Figure \ref{fig:modelstructure_vc}.
    \item \textbf{ImageCLIP}: CLIP pretrained on images. The original CLIP \parencite{radford2021learning} is used here. The whole model model structure is illustrated in Figure \ref{fig:modelstructure_ic}.
\end{enumerate}

Both models utilise CLIP and prompt engineering for action recognition, using videos and text prompts as input. The prompt template "A video of an animal <action>. This action is a kind of <category of action> behaviour." is used to generate the prompts for producing class embeddings. As shown in Figure \ref{fig:1_2_ClassEmbeddingInternVideo}, this approach can produce meaningful distributed class embeddings.

Before training begins, to accelerate the process, class embeddings of dimensions $(C, W)$ are pre-calculated using a text encoder pretrained by InternVideo. This is based on the prompt template, action name, and category, where $C$ is the number of classes and $W$ is the embedding length. This is pre-calculated since the text encoder is frozen, and the weights are not updated. 

Figure \ref{fig:modelstructure_vc} displays the model structure of VideoCLIP. To evaluate the structure fairly, two configurations are experimented with on this structure: VideoCLIP trained on all visual layers, and VideoCLIP trained solely on projection layers. Figure \ref{fig:modelstructure_ic} shows the model structure of ImageCLIP. To encode the set of 8 frames into video embeddings, 8 image embeddings produced by the image encoder of ImageCLIP are further passed through a 12-layer transformer followed by a pooling layer to produce a W-dimensional video embedding.

Given that this is a multilabel classification task, meaning that a video might belong to more than one class, an additional linear layer is added as a buffer to help the model fit better. Finally, the output logits of the linear layer are then processed through a sigmoid function to estimate the likelihood of each class.

\section{AFRICAN}
The whole AFRICAN consists of two stages: the pretraining stage and the action recognition stage. The pretraining stage is to train the model to distinguish whether two images are augmented from the same frame in a video or not. At the action recognition stage, the pretrained AFRICAN image encoder is applied to extract the embeddings for action recognition. 

\subsection{Pretraining Stage}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/charts/3_3_ConstrastiveSimilarityMatrix}
    \caption[Operation of pretraining AFRICAN]{Illustration of the operation during AFRICAN pretraining.}
f\label{fig:modelstructafsim}
\end{figure}

During the pretraining stage, AFRICAN applies contrastive learning techniques to distinguish whether two images are augmented from the same frame in a video or not. Figure \ref{fig:modelstructafsim} illustrates the batch operation of the model, which aligns with Figure \ref{fig:cliplossstructure} except for the input data. The input for forwarding, denoted by $x_i$, represents the $i$-th video sample in the dataset. $x_i$ is then copied twice into $x_{i1}$ and $x_{i2}$, each of which independently passes through the same video augmentation function $aug(\cdot)$ with different random states, as expressed in Equations \ref{eq:aug1} and \ref{eq:aug2}. 

\begin{equation}
    \label{eq:aug1}
    t_{i1} = aug(x_{i1})
\end{equation}
\begin{equation}
    \label{eq:aug2}
    t_{i2} = aug(x_{i2})
\end{equation}

After the transformation, $t_{i1}$ and $t_{i2}$ are separated into $j$ frames, denoted as $t_{i1}^j$ and $t_{i2}^j$. In this research, 8 frames are sampled from a video in a batch. Each frame will go through the same image encoder $f(\cdot)$ to be encoded into embeddings $e_{i1}^j$ and $e_{i2}^j$, as shown in Equations \ref{eq:enc1} and \ref{eq:enc2}.

\begin{equation}
    \label{eq:enc1}
    e_{i1}^j = f(t_{i1}^j)
\end{equation}
\begin{equation}
    \label{eq:enc2}
    e_{i2}^j = f(t_{i2}^j)
\end{equation}

Afterwards, the similarity $s$ between the image embeddings is calculated by normalisation and dot product with a trainable scale factor, as shown in Equation \ref{eq:sim}, where $k$ denotes the frame index of $e_{i1}$, $l$ denotes the frame index of $e_{i2}$, and $t$ dentoes the trainable scale factor. The numbers in the matrix in Figure \ref{fig:modelstructafsim} represent the learning targets for different image pair inputs. The diagonal line represents the frame pairs augmented from the same frames, while other numbers indicate pairs of images from different source frames. 

\begin{equation}
    \label{eq:sim}
    s_{i}^{kl} = norm(e_{i1}^{k}) \cdot norm(e_{i2}^{l}) \times exp(t)
\end{equation}




\subsection{Action Recognition Stage}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{assets/charts/3_4_ModelStructureAF}
    \caption[Operation of AFRICAN for action recognition]{This chart illustrates the operation of AFRICAN for action recognition.}
    \label{fig:modelstructaf_ar}
\end{figure}

Figure \ref{fig:modelstructaf_ar} illustrates the operation of AFRICAN for action recognition. The architecture consists of two streams: the upper is the pretrained CLIP stream, and the lower is the pretrained AFRICAN stream. 

Within the pretrained CLIP stream, both the visual encoder and class embedding are the same as those in ImageCLIP, as presented in Figure \ref{fig:modelstructure_ic}. Because the structure of ImageCLIP is able to achieve better performance than VideoCLIP, which will be discussed in Section \ref{sec:imageclipbetter}, ImageCLIP is employed here as the visual encoder in the pretrained CLIP stream. 

Contrastingly, within the pretrained AFRICAN stream, while the structure of the visual encoder is the same as those in the pretrained CLIP stream, it employs weights pretrained by AFRICAN. Given that AFRICAN is not pretrained by the vision-text multimodal model like CLIP, there is no corresponding class embedding. Therefore, a commonly used structure, the multilayer perceptron (MLP), is employed to map the embedding and one-hot encoding. The 3-layer MLP takes the video embedding as input and outputs 140-dimensional logits, which are then passed through a sigmoid function for classification.

After the logits, the input of the sigmoid function, are obtained in the two streams, they are used separately for prediction and computation of loss1 and loss2. Simultaneously, the logits of both streams are multiplied by W1 and W2, two 140-dimensional weights, respectively to determine the contributions of each class for both streams. This is followed by an addition to produce the final prediction and computation of loss3. Finally, loss1, loss2, and loss3 are added together to do the backpropagation. 
