\section{Why Does Image Clip perform better than Video Clip?}
\label{sec:discussion_vc}
\subsection{Domain Adaptation Gap}
VideoCLIP is pretrained on the Kinetics-400 dataset \parencite{kay2017kinetics}, which is a human action dataset, while ImageCLIP is pretrained on a combination of web-crawled and commonly used pre-existing image datasets. As suggested in \parencite{farahani2021brief}, the gap between the animal and human domains may lead to poor performance.

\subsection{Need for More Learnable Weights}
With the Animal Kingdom dataset containing more than 50 hours of animal video clips, it may require more trainable weights to achieve a better fit. To test the effect of different sizes of trainable weights, the following two settings are experimented with to add trainable weights:

\begin{enumerate}
    \item \textbf{VC\_AT}: The VC\_Proj model adds a 12-layer transformer, which is the same as the Image Clip settings in Figure \ref{fig:modelstructure_ic}. Rather than pooling all outputs of ViT's last layer, the embeddings of patches in the same frame are pooled together to obtain the frame-based embedding, which is used as input for the 12-layer transformer.
    \item \textbf{VC\_DF}: This setting is the same as the VC\_Proj model but with two more learnable layers. The Uniformer V2 model, which is the structure used by Invernvideo's video CLIP module, enhances the cross-frame relationship on the final 4 layers of ViT. There are two mechanisms to achieve this, Deep Position Embedding (DPE), a series of 3D CNN layers, and Feed Forward Network (FFN), the attention mechanism on the cls token with two linear layers. These two layers are learnable in this setting.
\end{enumerate}

Table \ref{tab:ablation_vc} and Figure \ref{fig:ablation_vc} show the results of the models with different learnable weights and the performance on each epoch. As illustrated in the figure, it is clear that the VC\_AT model is able to improve the VC\_Proj model to get closer to IC, proving that more learnable weights are indeed helpful for better fitting. Taking advantage of the well-designed mechanism of Uniformer V2, the VC\_DF model is able to achieve a higher score (55.98 mAP) than the IC model (54.36) as demonstrated in the table. 

\begin{table}[ht]
    \centering
    \caption[Training Results for Models with Different Learnable Weights] {This figure illustrates the training results for models with different learnable weights. \textbf{VC\_Vision} is VideoCLIP with vision layers learnable, as illustrated in Figure \ref{fig:modelstructure_vc} with a learnable video encoder. \textbf{VC\_Proj} is VideoCLIP with only projection layers learnable, as illustrated in Figure \ref{fig:modelstructure_vc} with a frozen video encoder. \textbf{IC} is Image CLIP trained on post-transformer layers, as illustrated in Figure \ref{fig:modelstructure_ic} with a frozen image encoder. \textbf{VC\_AT} is the VC\_Proj model that adds a 12-layer transformer. \textbf{VC\_DF}: is the same as the VC\_Proj model but with two more learnable layers ().}
    \label{tab:ablation_vc}
    \begin{tabular}{lllll}
        \toprule
        \multirow{2}{*}{Models} & Accuracy \\
        \cmidrule{2-2} 
        {} &  Best Epoch \\
        \midrule
        VC\_Vision & 25.74 \\
        VC\_Proj   & 48.56 \\
        IC         & 54.36 \\
        VC\_AT     & 52.79 \\
        VC\_DF     & 55.98 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[ht]
    \centering
    \resizebox{1.0\textwidth}{!}{\input{"assets/charts/5_1_AblationVC.pgf"}}
    \caption[mAP of VC\_Vision, VC\_Proj, IC, VC\_AT, VC\_DF on each Epoch]{This chart illustrates the mAP of VC\_Vision, VC\_Proj, IC, VC\_AT, VC\_DF on each epoch.}
    \label{fig:ablation_vc}
\end{figure}

\section{Why Does MSQNet perform better than AFRICAN?}
\label{sec:discussion_msqnet}

\subsection{Model Structure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{assets/charts_rw/MSQNet}
    \caption[Model Structure of MSQNet]{Illustration of MSQNet's model structure. Source \parencite{mondal2023msqnet}}
    \label{fig:discussion_msqnet}
\end{figure}

Figure \ref{fig:discussion_msqnet} presents the model structure of MSQNet. The core module of this structure is the Multi-Modal Transformer Decoder (MMTD). The entire structure can be divided into three parts: 

\begin{enumerate}
    \item \textbf{Key and Value of MMTD}: The input video $X \in \mathbf{R}^{N, T, C, H, W}$ is processed by the video encoder, specifically the TimeSformer, where $N$ represents the batch size, $T$ represents the number of frames, and C, H and W represents the channels, width and height of a frame, respectively. By averaging pooling patch embeddings generated by the video encoder within the same frame, frame-level embeddings are obtained. These embeddings are subsequently multiplied by the projection weights $W \in \mathbf{R}^{D \times D}$ to yield the key and value of the MMTD in the shape $(N, D)$. 
    \item \textbf{Query of MMTD}: The video embedding $Q_v \in \mathbf{R}^{N \times D}$ is derived from the average pooling of frame embeddings encoded by the image encoder of CLIP. The learnable class embedding $Q_l \in \mathbf{R}^{K \times D}$ is initialised with the text encoder of CLIP, taking the original label text as input, where $K$ denotes the number of classes. To produce a query $Q \in \mathbf{R}^{K \times D}$, the embeddings $Q_v$ and $Q_l$ are concatenated, resulting in $Q \in \mathbf{R}^{K \times 2D}$, which is then projected using the weight $W \in \mathbf{R}^{2D \times D}$. 
    \item \textbf{operation of MMTD}: Several layers of the cross-attention mechanism are applied to the query, key, and value to produce the embedding for each class $Q^l \in \mathbf{R}^{K \times D}$. This is then followed by an FFN module to produce the class logits.
\end{enumerate}

\subsection{Comparison}
Three key differences exist between AFRICAN and MSQNet: 

\begin{enumerate}
    \item \textbf{Cross-Attention}: The cross-attention mechanism enables the model to compute attention between the label embeddings and frame-level embedding, which is the most meticulously designed mechanism in this structure.
    \item \textbf{Learnable label Embedding}: Unlike AFRICAN, which uses fixed label embedding, MSQNet initialises the learnable label embedding with values produced by the pretrained CLIP model. This allows the model to update the weights of the label embedding to fit better.
    \item \textbf{Fusion of Image and Text Embedding}: Instead of adopting CLIP's similarity calculation method, MSQNet concatenates the text and video embeddings with a projection layer to fuse them together. 
\end{enumerate}

These features make MSQNet more adaptable to fit on a medium-sized dataset. On the contrary, AFRICAN tries to mimic the training scheme to directly calculate the image-text similarity for action recognition. Although this enables the model to update a minimum number of parameters to achieve an acceptable result, it restricts the model's potential to better fit the data. Given that MSQNet was published a month prior to the submission of this dissertation and their code is not yet publicly available, the improvement may be further studied in the future.

% \subsection{The effect smaller of batch size}
% Owing to the 80 Gb limitation of A100 GPU, I am able to train the fully learnable model, VC\_Vision, with a batch size of 16. To investigate the effect of this, I compare the 


% 0.514509499	0.543613255	0.559846222	0.257423162	0.481209993
% VC_AT	    IC	        VC_dd	    VC2_Vision	VC2_Proj
