This research aims to resolve two issues posing a huge challenge for deep learning models to recognise animal actions correctly: long-tail and temporal redundancy issues. To tackle the long-tail problem, the CLIP model with text prompt has been explored to solve this issue, as it effectively converts the learning target from the one-hot encoding to class embedding. Experiments in this research indicate that this method can significantly mitigate the long-tail issue and improve the model performance on the middle and tail classes. To address the temporal redundancy issue, AFRICAN is proposed, utilising the contrastive learning framework to make the model focus more on the subtle and fine-grained difference between frames in a video. The model was demonstrated to improve the performance on the action recognition task compared to the baseline model.
