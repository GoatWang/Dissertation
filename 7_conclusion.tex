This research aims to address two issues posing a huge challenge for deep learning models to recognise animal actions correctly: the long-tail issue and the temporal redundancy problem. To tackle the long-tail issue, the CLIP model with a text prompt has been explored, effectively transitioning the learning target from one-hot encoding to class embedding. Experiments indicate that this approach significantly mitigates the long-tail issue, enhancing the model's performance on middle and tail classes. To tackle the temporal redundancy problem, AFRICAN is proposed, utilising the contrastive learning framework to train the model to concentrate more on subtle and fine-grained differences between video frames. Both these enhancements contribute to an improved mAP in the animal action recognition task compared to the baseline model.

