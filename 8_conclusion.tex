In this research, I aim to resolve two issues posing a huge challenge for deep learning models to recognise animal actions correctly: long-tail and temporal redundancy issues. For the long-tail issue, the CLIP model with text prompt has been explored to solve this issue, as it effectively converts the learning target from the one-hot encoding to class embedding. My experiments indicate that this method can significantly mitigate the long-tail issue and improve the model performance on the middle and tail classes. For the temporal redundancy issue, I proposed the AFRICAN, utilising the contrastive learning framework to make the model focus more on the subtle and fine-grained difference between frames in a video. I also demonstrate the model with this ability can perform better on the action recognition task.