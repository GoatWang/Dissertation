This research aims to resolve two issues posing a huge challenge for deep learning models to recognise animal actions correctly: long-tail and temporal redundancy issues. For the long-tail issue, the CLIP model with text prompt has been explored to solve this issue, as it effectively converts the learning target from the one-hot encoding to class embedding. My experiments indicate that this method can significantly mitigate the long-tail issue and improve the model performance on the middle and tail classes. For the temporal redundancy issue, AFRICAN is proposed, utilising the contrastive learning framework to make the model focus more on the subtle and fine-grained difference between frames in a video. The model was demonstrated to improve the performance on the action recognition task.
