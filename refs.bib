@article{Greggor2019,
	author = {Greggor, Alison L. and Blumstein, Daniel T. and Wong, Bob B. M. and Berger-Tal, Oded},
	date = {2019},
	date-added = {2023-08-20 11:08:11 +0100},
	date-modified = {2023-08-20 11:08:11 +0100},
	doi = {10.1186/s13750-019-0164-4},
	id = {Greggor2019},
	journal = {Environmental Evidence},
	number = {1},
	pages = {23},
	title = {Using animal behavior in conservation management: a series of systematic reviews and maps},
	url = {https://doi.org/10.1186/s13750-019-0164-4},
	volume = {8},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1186/s13750-019-0164-4}
}

@inproceedings{singh2020animal,
  title={Animal detection in man-made environments},
  author={Singh, Abhineet and Pietrasik, Marcin and Natha, Gabriell and Ghouaiel, Nehla and Brizel, Ken and Ray, Nilanjan},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1438--1449},
  year={2020}
}

@inproceedings{ng2022animal,
  title={Animal kingdom: A large and diverse dataset for animal behavior understanding},
  author={Ng, Xun Long and Ong, Kian Eng and Zheng, Qichen and Ni, Yun and Yeo, Si Yong and Liu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19023--19034},
  year={2022}
}

@article{Giersberg:2022aa,
	abstract = {Video analysis is a popular and frequently used tool in animal behavior and welfare research. In addition to the actual object of research, video recordings often provide unforeseen information about the progress of the study, the animals or the people involved. Conflicts can arise when this information is weighed against the original intention of the recordings and broader social expectations. Uncertainty may prevent the video observers, often less experienced researchers, to properly address these conflicts, which can pose a threat to animal welfare and research quality and integrity. In this article, we aim to raise awareness of the interrelationship of variables characteristic for video-based animal studies and the potential conflicts emerging from this. We propose stepping stones for a framework which enables a culture of openness in dealing with unexpected and unintended events observed during video analysis. As a basis, a frame of reference regarding privacy and duty of care toward animals should be created and shared with all persons involved. At this stage, expectations and responsibilities need to be made explicit. During running and reporting of the study, the risk of animal welfare and research integrity issues can be mitigated by making conflicts discussible and offering realistic opportunities on how to deal with them. A practice which is outlined and guided by conversation will prevent a mere compliance-based approach centered on checklists and decision trees. Based on these stepping stones, educational material can be produced to foster reflection, co-creation and application of ethical practice.},
	address = {Animals in Science and Society, Department Population Health Sciences, Faculty of Veterinary Medicine, Utrecht University, Utrecht, Netherlands.; Animals in Science and Society, Department Population Health Sciences, Faculty of Veterinary Medicine, Utrecht University, Utrecht, Netherlands.},
	author = {Giersberg, Mona F and Meijboom, Franck L B},
	cois = {The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.},
	copyright = {Copyright {\copyright}2022 Giersberg and Meijboom.},
	crdt = {2022/05/13 11:21},
	date = {2022},
	date-added = {2023-08-20 11:15:13 +0100},
	date-modified = {2023-08-20 11:15:13 +0100},
	dep = {20220425},
	doi = {10.3389/fvets.2022.864677},
	edat = {2022/05/14 06:00},
	issn = {2297-1769 (Print); 2297-1769 (Electronic); 2297-1769 (Linking)},
	jid = {101666658},
	journal = {Front Vet Sci},
	jt = {Frontiers in veterinary science},
	keywords = {animal study; co-creation; participatory approach; privacy; research integrity; video},
	language = {eng},
	lid = {10.3389/fvets.2022.864677 {$[$}doi{$]$}; 864677},
	lr = {20220716},
	mhda = {2022/05/14 06:01},
	oto = {NOTNLM},
	own = {NLM},
	pages = {864677},
	phst = {2022/01/28 00:00 {$[$}received{$]$}; 2022/03/28 00:00 {$[$}accepted{$]$}; 2022/05/13 11:21 {$[$}entrez{$]$}; 2022/05/14 06:00 {$[$}pubmed{$]$}; 2022/05/14 06:01 {$[$}medline{$]$}},
	pl = {Switzerland},
	pmc = {PMC9082409},
	pmid = {35548048},
	pst = {epublish},
	pt = {Journal Article; Review},
	status = {PubMed-not-MEDLINE},
	title = {Caught on Camera: On the Need of Responsible Use of Video Observation for Animal Behavior and Welfare Research.},
	volume = {9},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.3389/fvets.2022.864677}
}

@INPROCEEDINGS{8259762,
  author={Nguyen, Hung and Maclagan, Sarah J. and Nguyen, Tu Dinh and Nguyen, Thin and Flemons, Paul and Andrews, Kylie and Ritchie, Euan G. and Phung, Dinh},
  booktitle={2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Animal Recognition and Identification with Deep Convolutional Neural Networks for Automated Wildlife Monitoring}, 
  year={2017},
  volume={},
  number={},
  pages={40-49},
  doi={10.1109/DSAA.2017.31}
}

@article{ANDERSON201418,
	abstract = {The new field of ``Computational Ethology'' is made possible by advances in technology, mathematics, and engineering that allow scientists to automate the measurement and the analysis of animal behavior. We explore the opportunities and long-term directions of research in this area.},
	author = {David J. Anderson and Pietro Perona},
	doi = {https://doi.org/10.1016/j.neuron.2014.09.005},
	issn = {0896-6273},
	journal = {Neuron},
	number = {1},
	pages = {18-31},
	title = {Toward a Science of Computational Ethology},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627314007934},
	volume = {84},
	year = {2014},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0896627314007934},
	bdsk-url-2 = {https://doi.org/10.1016/j.neuron.2014.09.005}
}

@article{mondal2023msqnet,
  title={MSQNet: Actor-agnostic Action Recognition with Multi-modal Query},
  author={Mondal, Anindya and Nag, Sauradip and Prada, Joaquin M and Zhu, Xiatian and Dutta, Anjan},
  journal={arXiv preprint arXiv:2307.10763},
  year={2023}
}

@article{cao2019learning,
  title={Learning imbalanced datasets with label-distribution-aware margin loss},
  author={Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{zhang2021videolt,
  title={Videolt: Large-scale long-tailed video recognition},
  author={Zhang, Xing and Wu, Zuxuan and Weng, Zejia and Fu, Huazhu and Chen, Jingjing and Jiang, Yu-Gang and Davis, Larry S},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7960--7969},
  year={2021}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{wang2022internvideo,
  title={Internvideo: General video foundation models via generative and discriminative learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}

@inproceedings{ma2022x,
  title={X-clip: End-to-end multi-grained contrastive learning for video-text retrieval},
  author={Ma, Yiwei and Xu, Guohai and Sun, Xiaoshuai and Yan, Ming and Zhang, Ji and Ji, Rongrong},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={638--647},
  year={2022}
}

@article{YUAN2018221,
	abstract = {Recognizing human actions in videos is a challenging problem owning to complex motion appearance, various backgrounds and semantic gap between low-level features and high-level semantics. Existing methods have scored some achievements and many new thoughts have been proposed for action recognition. They focus on designing a robust feature description and training an elaborate learning model, and many of them can benefit from a two-stream network with a stack of RGB frames and optical flow frames. However, these features for human action representation are struggling with the limited feature representation as RGB videos are confused by static appearance redundancy and optical flow videos cannot represent the detailed appearance. To solve these problems, we propose an efficient algorithm based on the spatial-optical data organization and the sequential learning framework. There are two contributions of our method: a novel data organization based on hierarchical weighting segmentation and optical flow for video representation, and a lightweight deep learning model based on the Convolutional 3D (C3D) network and the Recurrent Neural Network (RNN) for complicated action recognition. The new data organization aggregates the merits of motion appearance, movement trajectories and optical flow in a creative way to highlight the meaningful information. And the proposed lightweight model has an insight into patterns and semantics of sequential data by low-level spatiotemporal feature extraction and high-level information mining. The proposed method is evaluated on the state-of-the-art dataset and the results demonstrate that our method have a good performance for complex human action recognition.},
	author = {Yuan Yuan and Yang Zhao and Qi Wang},
	doi = {https://doi.org/10.1016/j.neucom.2018.06.071},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Action recognition, Spatiotemporal feature, Deep learning, Sequential learning framework},
	pages = {221-233},
	title = {Action recognition using spatial-optical data organization and sequential learning framework},
	url = {https://www.sciencedirect.com/science/article/pii/S092523121830849X},
	volume = {315},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S092523121830849X},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2018.06.071}
}

@article{li2022uniformer,
  title={Uniformer: Unified transformer for efficient spatiotemporal representation learning},
  author={Li, Kunchang and Wang, Yali and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2201.04676},
  year={2022}
}

@article{khan2017cost,
  title={Cost-sensitive learning of deep feature representations from imbalanced data},
  author={Khan, Salman H and Hayat, Munawar and Bennamoun, Mohammed and Sohel, Ferdous A and Togneri, Roberto},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={8},
  pages={3573--3587},
  year={2017},
  publisher={IEEE}
}

@inproceedings{mostajabi2015feedforward,
  title={Feedforward semantic segmentation with zoom-out features},
  author={Mostajabi, Mohammadreza and Yadollahpour, Payman and Shakhnarovich, Gregory},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3376--3385},
  year={2015}
}

@inproceedings{cui2019class,
  title={Class-balanced loss based on effective number of samples},
  author={Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9268--9277},
  year={2019}
}

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}

@inproceedings{shen2016relay,
  title={Relay backpropagation for effective learning of deep convolutional neural networks},
  author={Shen, Li and Lin, Zhouchen and Huang, Qingming},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part VII 14},
  pages={467--482},
  year={2016},
  organization={Springer}
}

@ARTICLE{5128907,
  author={He, Haibo and Garcia, Edwardo A.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Learning from Imbalanced Data}, 
  year={2009},
  volume={21},
  number={9},
  pages={1263-1284},
  doi={10.1109/TKDE.2008.239}
}

@inproceedings{mahajan2018exploring,
  title={Exploring the limits of weakly supervised pretraining},
  author={Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and Van Der Maaten, Laurens},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={181--196},
  year={2018}
}

@inproceedings{zhou2020bbn,
  title={Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition},
  author={Zhou, Boyan and Cui, Quan and Wei, Xiu-Shen and Chen, Zhao-Min},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9719--9728},
  year={2020}
}

@article{kang2019decoupling,
  title={Decoupling representation and classifier for long-tailed recognition},
  author={Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng and Gordo, Albert and Feng, Jiashi and Kalantidis, Yannis},
  journal={arXiv preprint arXiv:1910.09217},
  year={2019}
}

@article{liu2022long,
  title={Long-tailed Recognition by Learning from Latent Categories},
  author={Liu, Weide and Wu, Zhonghua and Wang, Yiming and Ding, Henghui and Liu, Fayao and Lin, Jie and Lin, Guosheng},
  journal={arXiv preprint arXiv:2206.01010},
  year={2022}
}

@inproceedings{perrett2023use,
  title={Use Your Head: Improving Long-Tail Video Recognition},
  author={Perrett, Toby and Sinha, Saptarshi and Burghardt, Tilo and Mirmehdi, Majid and Damen, Dima},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2415--2425},
  year={2023}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@article{nichol2018first,
title={On first-order meta-learning algorithms},
author={Nichol, Alex and Achiam, Joshua and Schulman, John},
journal={arXiv preprint arXiv:1803.02999},
year={2018}
}

@article{2018Reptile,
	author = {Nichol, Alex and Schulman, John},
	month = {03},
	title = {Reptile: a Scalable Metalearning Algorithm},
	year = {2018}
}

@article{andrychowicz2016learning,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{li2020dada,
  title={Dada: Differentiable automatic data augmentation},
  author={Li, Yonggang and Hu, Guosheng and Wang, Yongtao and Hospedales, Timothy and Robertson, Neil M and Yang, Yongxin},
  journal={arXiv preprint arXiv:2003.03780},
  year={2020}
}

@article{galashov2022data,
  title={Data augmentation for efficient learning from parametric experts},
  author={Galashov, Alexandre and Merel, Josh S and Heess, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31484--31496},
  year={2022}
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@article{shu2019meta,
  title={Meta-weight-net: Learning an explicit mapping for sample weighting},
  author={Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{NIPS2017_147ebe63,
	author = {Wang, Yu-Xiong and Ramanan, Deva and Hebert, Martial},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Learning to Model the Tail},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/147ebe637038ca50a1265abac8dea181-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/147ebe637038ca50a1265abac8dea181-Paper.pdf}
}

@inproceedings{liu2019large,
  title={Large-scale long-tailed recognition in an open world},
  author={Liu, Ziwei and Miao, Zhongqi and Zhan, Xiaohang and Wang, Jiayun and Gong, Boqing and Yu, Stella X},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2537--2546},
  year={2019}
}

@InProceedings{Zhu_2020_CVPR,
	author = {Zhu, Linchao and Yang, Yi},
	title = {Inflated Episodic Memory With Region Self-Attention for Long-Tailed Visual Recognition},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2020}
}

@inproceedings{li2021metasaug,
  title={Metasaug: Meta semantic augmentation for long-tailed visual recognition},
  author={Li, Shuang and Gong, Kaixiong and Liu, Chi Harold and Wang, Yulin and Qiao, Feng and Cheng, Xinjing},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5212--5221},
  year={2021}
}

@InProceedings{Fu_2022_ACCV,
    author    = {Fu, Siming and Chu, Huanpeng and He, Xiaoxuan and Wang, Hualiang and Yang, Zhenyu and Hu, Haoji},
    title     = {Meta-Prototype Decoupled Training  for Long-tailed Learning},
    booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
    month     = {December},
    year      = {2022},
    pages     = {569-585}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhang2022tip,
  title={Tip-adapter: Training-free adaption of clip for few-shot classification},
  author={Zhang, Renrui and Zhang, Wei and Fang, Rongyao and Gao, Peng and Li, Kunchang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  booktitle={European Conference on Computer Vision},
  pages={493--510},
  year={2022},
  organization={Springer}
}

@inproceedings{wang2022cris,
  title={Cris: Clip-driven referring image segmentation},
  author={Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11686--11695},
  year={2022}
}

@article{lin2023gridclip,
  title={GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning},
  author={Lin, Jiayi and Gong, Shaogang},
  journal={arXiv preprint arXiv:2303.09252},
  year={2023}
}

@article{mokady2021clipcap,
  title={Clipcap: Clip prefix for image captioning},
  author={Mokady, Ron and Hertz, Amir and Bermano, Amit H},
  journal={arXiv preprint arXiv:2111.09734},
  year={2021}
}

@inproceedings{xu-etal-2021-videoclip,
    title = "{V}ideo{CLIP}: Contrastive Pre-training for Zero-shot Video-Text Understanding",
    author = "Xu, Hu  and
      Ghosh, Gargi  and
      Huang, Po-Yao  and
      Okhonko, Dmytro  and
      Aghajanyan, Armen  and
      Metze, Florian  and
      Zettlemoyer, Luke  and
      Feichtenhofer, Christoph",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.544",
    doi = "10.18653/v1/2021.emnlp-main.544",
    pages = "6787--6800",
    abstract = "We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT.",
}

@inproceedings{10.1007/978-3-030-12939-2_20,
	abstract = {Most of the top performing action recognition methods use optical flow as a ``black box'' input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: (1) optical flow is useful for action recognition because it is invariant to appearance, (2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, (3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, (4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and (5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.},
	address = {Cham},
	author = {Sevilla-Lara, Laura and Liao, Yiyi and G{\"u}ney, Fatma and Jampani, Varun and Geiger, Andreas and Black, Michael J.},
	booktitle = {Pattern Recognition},
	editor = {Brox, Thomas and Bruhn, Andr{\'e}s and Fritz, Mario},
	isbn = {978-3-030-12939-2},
	pages = {281--297},
	publisher = {Springer International Publishing},
	title = {On the Integration of Optical Flow and Action Recognition},
	year = {2019}
}

@InProceedings{Piergiovanni_2019_CVPR,
	author = {Piergiovanni, AJ and Ryoo, Michael S.},
	title = {Representation Flow for Action Recognition},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2019}
}

@inproceedings{sevilla2019integration,
  title={On the integration of optical flow and action recognition},
  author={Sevilla-Lara, Laura and Liao, Yiyi and G{\"u}ney, Fatma and Jampani, Varun and Geiger, Andreas and Black, Michael J},
  booktitle={Pattern Recognition: 40th German Conference, GCPR 2018, Stuttgart, Germany, October 9-12, 2018, Proceedings 40},
  pages={281--297},
  year={2019},
  organization={Springer}
}

@InProceedings{Lee_2018_ECCV,
	author = {Lee, Myunggi and Lee, Seungeui and Son, Sungjoon and Park, Gyutae and Kwak, Nojun},
	title = {Motion Feature Network: Fixed Motion Filter for Action Recognition},
	booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
	month = {September},
	year = {2018}
}

@INPROCEEDINGS{8354283,
  author={Ng, Joe Yue-Hei and Choi, Jonghyun and Neumann, Jan and Davis, Larry S.},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={ActionFlowNet: Learning Motion Representation for Action Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={1616-1624},
  doi={10.1109/WACV.2018.00179}
}

@article{10.1145/3577925,
	author = {Schiappa, Madeline C. and Rawat, Yogesh S. and Shah, Mubarak},
	title = {Self-Supervised Learning for Videos: A Survey},
	year = {2023},
	issue_date = {December 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {55},
	number = {13s},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3577925},
	doi = {10.1145/3577925},
	abstract = {The remarkable success of deep learning in various domains relies on the availability of large-scale annotated datasets. However, obtaining annotations is expensive and requires great effort, which is especially challenging for videos. Moreover, the use of human-generated annotations leads to models with biased learning and poor domain generalization and robustness. As an alternative, self-supervised learning provides a way for representation learning that does not require annotations and has shown promise in both image and video domains. In contrast to the image domain, learning video representations are more challenging due to the temporal dimension, bringing in motion and other environmental dynamics. This also provides opportunities for video-exclusive ideas that advance self-supervised learning in the video and multimodal domains. In this survey, we provide a review of existing approaches on self-supervised learning focusing on the video domain. We summarize these methods into four different categories based on their learning objectives: (1) pretext tasks, (2) generative learning, (3) contrastive learning, and (4) cross-modal agreement. We further introduce the commonly used datasets, downstream evaluation tasks, insights into the limitations of existing works, and the potential future directions in this area.},
	journal = {ACM Comput. Surv.},
	month = {jul},
	articleno = {288},
	numpages = {37},
	keywords = {video understanding, visual-language models, representation learning, Self-supervised learning, multimodal learning, deep learning, zero-shot learning}
}

@InProceedings{Wang_2019_CVPR,
	author = {Wang, Jiangliu and Jiao, Jianbo and Bao, Linchao and He, Shengfeng and Liu, Yunhui and Liu, Wei},
	title = {Self-Supervised Spatio-Temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2019}
}

@article{DBLP:journals/corr/abs-1811-11387,
  author       = {Longlong Jing and
                  Yingli Tian},
  title        = {Self-supervised Spatiotemporal Feature Learning by Video Geometric
                  Transformations},
  journal      = {CoRR},
  volume       = {abs/1811.11387},
  year         = {2018},
  url          = {http://arxiv.org/abs/1811.11387},
  eprinttype    = {arXiv},
  eprint       = {1811.11387},
  timestamp    = {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1811-11387.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Yao_2020_CVPR,
	author = {Yao, Yuan and Liu, Chang and Luo, Dezhao and Zhou, Yu and Ye, Qixiang},
	title = {Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2020}
}

@inproceedings{10.1007/978-3-030-58520-4_30,
	abstract = {This paper addresses the problem of self-supervised video representation learning from a new perspective -- by video pace prediction. It stems from the observation that human visual system is sensitive to video pace, e.g., slow motion, a widely used technique in film making. Specifically, given a video played in natural pace, we randomly sample training clips in different paces and ask a neural network to identify the pace for each video clip. The assumption here is that the network can only succeed in such a pace reasoning task when it understands the underlying video content and learns representative spatio-temporal features. In addition, we further introduce contrastive learning to push the model towards discriminating different paces by maximizing the agreement on similar video content. To validate the effectiveness of the proposed method, we conduct extensive experiments on action recognition and video retrieval tasks with several alternative network architectures. Experimental evaluations show that our approach achieves state-of-the-art performance for self-supervised video representation learning across different network architectures and different benchmarks. The code and pre-trained models are available at https://github.com/laura-wang/video-pace.},
	address = {Cham},
	author = {Wang, Jiangliu and Jiao, Jianbo and Liu, Yun-Hui},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58520-4},
	pages = {504--521},
	publisher = {Springer International Publishing},
	title = {Self-supervised Video Representation Learning by Pace Prediction},
	year = {2020}
}

@inproceedings{10.1007/978-3-030-58604-1_26,
	abstract = {We introduce a novel self-supervised learning approach to learn representations of videos that are responsive to changes in the motion dynamics. Our representations can be learned from data without human annotation and provide a substantial boost to the training of neural networks on small labeled data sets for tasks such as action recognition, which require to accurately distinguish the motion of objects. We promote an accurate learning of motion without human annotation by training a neural network to discriminate a video sequence from its temporally transformed versions. To learn to distinguish non-trivial motions, the design of the transformations is based on two principles: 1) To define clusters of motions based on time warps of different magnitude; 2) To ensure that the discrimination is feasible only by observing and analyzing as many image frames as possible. Thus, we introduce the following transformations: forward-backward playback, random frame skipping, and uniform frame skipping. Our experiments show that networks trained with the proposed method yield representations with improved transfer performance for action recognition on UCF101 and HMDB51.},
	address = {Cham},
	author = {Jenni, Simon and Meishvili, Givi and Favaro, Paolo},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58604-1},
	pages = {425--442},
	publisher = {Springer International Publishing},
	title = {Video Representation Learning by Recognizing Temporal Transformations},
	year = {2020}
}


@InProceedings{pmlr-v119-chen20j,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5 top-1 accuracy, which is a 7 relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1 of the labels, we achieve 85.8 top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}

@InProceedings{Feichtenhofer_2021_CVPR,
    author    = {Feichtenhofer, Christoph and Fan, Haoqi and Xiong, Bo and Girshick, Ross and He, Kaiming},
    title     = {A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3299-3309}
}

@misc{oord2019representation,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{kay2017kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv preprint arXiv:1705.06950},
  year={2017}
}

@article{farahani2021brief,
  title={A brief review of domain adaptation},
  author={Farahani, Abolfazl and Voghoei, Sahar and Rasheed, Khaled and Arabnia, Hamid R},
  journal={Advances in data science and information engineering: proceedings from ICDATA 2020 and IKE 2020},
  pages={877--894},
  year={2021},
  publisher={Springer}
}

@inproceedings{huang2022flowformer,
  title={Flowformer: A transformer architecture for optical flow},
  author={Huang, Zhaoyang and Shi, Xiaoyu and Zhang, Chao and Wang, Qiang and Cheung, Ka Chun and Qin, Hongwei and Dai, Jifeng and Li, Hongsheng},
  booktitle={European Conference on Computer Vision},
  pages={668--685},
  year={2022},
  organization={Springer}
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6299--6308},
  year={2017}
}

@inproceedings{tran2015learning,
  title={Learning spatiotemporal features with 3d convolutional networks},
  author={Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4489--4497},
  year={2015}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{bertasius2021space,
  title={Is space-time attention all you need for video understanding?},
  author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle={ICML},
  volume={2},
  number={3},
  pages={4},
  year={2021}
}

@inproceedings{feichtenhofer2019slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6202--6211},
  year={2019}
}

@article{li2022uniformerv2,
  title={Uniformerv2: Spatiotemporal learning by arming image vits with video uniformer},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2211.09552},
  year={2022}
}

@inproceedings{cui2022contrastive,
  title={Contrastive vision-language pre-training with limited resources},
  author={Cui, Quan and Zhou, Boyan and Guo, Yu and Yin, Weidong and Wu, Hao and Yoshie, Osamu and Chen, Yubo},
  booktitle={European Conference on Computer Vision},
  pages={236--253},
  year={2022},
  organization={Springer}
}